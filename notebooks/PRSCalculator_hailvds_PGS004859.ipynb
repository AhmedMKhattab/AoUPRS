{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgreen; padding: 10px; font-size: 24px;\">\n",
    "    \n",
    "__PRS Calculator:__ Hail VDS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgrey; padding: 10px;; font-size: 18px;\">  \n",
    "    \n",
    "__Author:__ Ahmed Khattab  \n",
    "        __Scripps Research__\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightblue; padding: 10px; font-size: 16px;\"> \n",
    "    \n",
    "__Introduction__\n",
    "\n",
    "In this notebook, we will demonstrate how to calculate Polygenic Risk Scores (PRS) using the Hail MatrixTable (MT) data structure.\n",
    "\n",
    "\n",
    "__Using Hail VDS__ \n",
    "\n",
    "VDS is a sparse Hail format that contains the complete callset.\n",
    "\n",
    "__Resources used?__   \n",
    "\n",
    "\n",
    "Cost when running: $5.57 per hour  \n",
    "\n",
    "Main node: 8CPUs, 52GB RAM, 150 GB Disk  \n",
    "Workers (2/50): 4CPUs, 15GB RAM, 150GB Disk    \n",
    "\n",
    "Time and Cost:  __$7.4/ 80min__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date: 2024-06-25\n",
      "Start time: 01:56:20\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Record the start time\n",
    "current_date = start_time.date()\n",
    "current_time = start_time.time()\n",
    "\n",
    "# Format the current date\n",
    "formatted_start_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Format the current time\n",
    "formatted_start_time = current_time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# Print the formatted date and time separately\n",
    "print(\"Start date:\", formatted_start_date)\n",
    "print(\"Start time:\", formatted_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/.local/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T20:09:11.806426Z",
     "iopub.status.busy": "2023-09-16T20:09:11.805899Z",
     "iopub.status.idle": "2023-09-16T20:09:12.059189Z",
     "shell.execute_reply": "2023-09-16T20:09:12.058207Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gcsfs\n",
    "import multiprocessing\n",
    "import ast\n",
    "import concurrent.futures\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T20:09:12.080061Z",
     "iopub.status.busy": "2023-09-16T20:09:12.079227Z",
     "iopub.status.idle": "2023-09-16T20:09:30.186923Z",
     "shell.execute_reply": "2023-09-16T20:09:30.185873Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a47d28da-d3ab-44a0-a431-402d2529b80d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"a47d28da-d3ab-44a0-a431-402d2529b80d\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"a47d28da-d3ab-44a0-a431-402d2529b80d\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a47d28da-d3ab-44a0-a431-402d2529b80d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a47d28da-d3ab-44a0-a431-402d2529b80d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hail/context.py:352: UserWarning:\n",
      "\n",
      "Using hl.init with a default_reference argument is deprecated. To set a default reference genome after initializing hail, call `hl.default_reference` with an argument to set the default reference genome.\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-7093-m.c.terra-vpc-sc-e098d676.internal:39297\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130-bea04d9c79b5\n",
      "LOGGING: writing to /home/jupyter/workspaces/duplicateoftype2diabetesriskprediction/hail-20240625-0156-0.2.130-bea04d9c79b5.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init(tmp_dir='hail_temp/', default_reference='GRCh38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.getenv(\"WORKSPACE_BUCKET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Hail VDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/vds/hail.vds'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_srwgs_path = os.getenv(\"WGS_VDS_PATH\")\n",
    "vds_srwgs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = hl.vds.read_vds(vds_srwgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245394"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds.n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Flagged srWGS samples\n",
    "\n",
    "AoU provides a table listing samples that are flagged as part of the sample outlier QC for the srWGS SNP and Indel joint callset. \n",
    "\n",
    "__Read more:__ https://support.researchallofus.org/hc/en-us/articles/4614687617556-How-the-All-of-Us-Genomic-data-are-organized#h_01GY7QZR2QYFDKGK89TCHSJSA7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read flagged samples\n",
    "flagged_samples_path = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -u $$GOOGLE_PROJECT cat $flagged_samples_path > flagged_samples.cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 01:57:48.296 Hail: INFO: Reading table without type imputation1) / 1]\n",
      "  Loading field 'sample_id' as type str (not specified)\n"
     ]
    }
   ],
   "source": [
    "# Import flagged samples into a hail table\n",
    "flagged_samples = hl.import_table(flagged_samples_path, key='sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Drop flagged sample from main Hail VDS\n",
    "vds_no_flag = hl.vds.filter_samples(vds, flagged_samples, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_no_flag.n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Sample Intended for PRS Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pre-selected sample for all people with WGS and EHR data availabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78df7154eb74258ac3b66031bb20416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/206173 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "\n",
    "# This query represents dataset \"participants_with_WGS_EHR_phenotypes_020524\" for domain \"person\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_16967016_person_sql = \"\"\"\n",
    "    SELECT\n",
    "        person.person_id,\n",
    "        person.gender_concept_id,\n",
    "        p_gender_concept.concept_name as gender,\n",
    "        person.birth_datetime as date_of_birth,\n",
    "        person.race_concept_id,\n",
    "        p_race_concept.concept_name as race,\n",
    "        person.ethnicity_concept_id,\n",
    "        p_ethnicity_concept.concept_name as ethnicity,\n",
    "        person.sex_at_birth_concept_id,\n",
    "        p_sex_at_birth_concept.concept_name as sex_at_birth \n",
    "    FROM\n",
    "        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".person` person \n",
    "    LEFT JOIN\n",
    "        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".concept` p_gender_concept \n",
    "            ON person.gender_concept_id = p_gender_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".concept` p_race_concept \n",
    "            ON person.race_concept_id = p_race_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".concept` p_ethnicity_concept \n",
    "            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".concept` p_sex_at_birth_concept \n",
    "            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  \n",
    "    WHERE\n",
    "        person.PERSON_ID IN (\n",
    "            SELECT\n",
    "                distinct person_id  \n",
    "            FROM\n",
    "                `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".cb_search_person` cb_search_person  \n",
    "            WHERE\n",
    "                cb_search_person.person_id IN (\n",
    "                    SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_ehr_data = 1 \n",
    "                ) \n",
    "                AND cb_search_person.person_id IN (\n",
    "                    SELECT\n",
    "                        person_id \n",
    "                    FROM\n",
    "                        `\"\"\" + os.environ[\"WORKSPACE_CDR\"] + \"\"\".cb_search_person` p \n",
    "                    WHERE\n",
    "                        has_whole_genome_variant = 1 \n",
    "                ) \n",
    "            )\"\"\"\n",
    "\n",
    "dataset_16967016_person_df = pandas.read_gbq(\n",
    "    dataset_16967016_person_sql,\n",
    "    dialect=\"standard\",\n",
    "    use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "    progress_bar_type=\"tqdm_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206173"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_16967016_person_df['person_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = dataset_16967016_person_df['person_id'].unique()\n",
    "allofus_id = pd.DataFrame(unique_ids, columns=['person_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to the bucket\n",
    "allofus_id.to_csv(f'{bucket}/prs_calculator_tutorial/prs_calculator_hail_vds/people_with_WGS_EHR_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 01:58:05.637 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'person_id' as type str (not specified)\n"
     ]
    }
   ],
   "source": [
    "sample_needed_ht = hl.import_table(f'{bucket}/prs_calculator_tutorial/prs_calculator_hail_vds/people_with_WGS_EHR_ids.csv', delimiter=',', key='person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Filter samples\n",
    "vds_subset = hl.vds.filter_samples(vds_no_flag, sample_needed_ht, keep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_subset.n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PRS Weight Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using PGS004859 (www.pgscatalog.org/score/PGS004859/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read table\n",
    "with gcsfs.GCSFileSystem().open(f'{bucket}/prs_calculator_tutorial/prs_calculator_hail_vds/vat_check/PGS004859_Deutsch_AJ_PRS_1108235_Type_2_diabetes_T2D_Diabetes_Care_2023.GRCh37_to_GRCh38.csv', 'rb') as gcs_file:\n",
    "    prs_df = pd.read_csv(gcs_file)\n",
    "\n",
    "# change columns names to fit Hail\n",
    "prs_df['contig'] = 'chr' + prs_df['chr'].astype(str)\n",
    "prs_df['position'] = prs_df['bp']\n",
    "prs_df['variant_id'] = prs_df.apply(lambda row: f'{row[\"contig\"]}:{row[\"position\"]}', axis=1)\n",
    "\n",
    "hail_df_fp = f\"{bucket}/prs_calculator_tutorial/prs_calculator_hail_vds/vat_check/PGS004859_weights_tabel.csv\"\n",
    "prs_df.to_csv(hail_df_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gcsfs.GCSFileSystem().open(f'{bucket}/prs_calculator_tutorial/prs_calculator_hail_vds/vat_check/PGS004859_weights_tabel.csv', 'rb') as gcs_file:\n",
    "    PGS004859_weights_tabel = pd.read_csv(gcs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107037, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PGS004859_weights_tabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>bp</th>\n",
       "      <th>rs_number</th>\n",
       "      <th>effect_allele</th>\n",
       "      <th>noneffect_allele</th>\n",
       "      <th>weight</th>\n",
       "      <th>additive</th>\n",
       "      <th>recessive</th>\n",
       "      <th>dominant</th>\n",
       "      <th>contig</th>\n",
       "      <th>position</th>\n",
       "      <th>variant_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>818802</td>\n",
       "      <td>1:754182:G:A:rs3131969</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chr1</td>\n",
       "      <td>818802</td>\n",
       "      <td>chr1:818802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>833068</td>\n",
       "      <td>1:768448:A:G:rs12562034</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chr1</td>\n",
       "      <td>833068</td>\n",
       "      <td>chr1:833068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1104646</td>\n",
       "      <td>1:1040026:C:T:rs6671356</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1104646</td>\n",
       "      <td>chr1:1104646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1106320</td>\n",
       "      <td>1:1041700:G:A:rs6604968</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1106320</td>\n",
       "      <td>chr1:1106320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1113575</td>\n",
       "      <td>1:1048955:G:A:rs4970405</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1113575</td>\n",
       "      <td>chr1:1113575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chr       bp                rs_number effect_allele noneffect_allele  \\\n",
       "0    1   818802   1:754182:G:A:rs3131969             G                A   \n",
       "1    1   833068  1:768448:A:G:rs12562034             A                G   \n",
       "2    1  1104646  1:1040026:C:T:rs6671356             C                T   \n",
       "3    1  1106320  1:1041700:G:A:rs6604968             G                A   \n",
       "4    1  1113575  1:1048955:G:A:rs4970405             G                A   \n",
       "\n",
       "     weight  additive  recessive  dominant contig  position    variant_id  \n",
       "0  0.000006         1          0         0   chr1    818802   chr1:818802  \n",
       "1  0.000012         1          0         0   chr1    833068   chr1:833068  \n",
       "2  0.000086         1          0         0   chr1   1104646  chr1:1104646  \n",
       "3  0.000088         1          0         0   chr1   1106320  chr1:1106320  \n",
       "4  0.000019         1          0         0   chr1   1113575  chr1:1113575  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PGS004859_weights_tabel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRS Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     5,
     28
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import hail as hl\n",
    "import gcsfs\n",
    "\n",
    "def calculate_effect_allele_count_na_hom_ref(mt_subset):\n",
    "    effect_allele = mt_subset.prs_info['effect_allele']\n",
    "    non_effect_allele = mt_subset.prs_info['noneffect_allele']\n",
    "        \n",
    "    ref_allele = mt_subset.alleles[0]\n",
    "\n",
    "    # Create a set of alternate alleles using hl.set\n",
    "    alt_alleles_set = hl.set(mt_subset.alleles[1:].map(lambda allele: allele))\n",
    "\n",
    "    is_effect_allele_ref = ref_allele == effect_allele\n",
    "    is_effect_allele_alt = alt_alleles_set.contains(effect_allele)\n",
    "    is_non_effect_allele_ref = ref_allele == non_effect_allele\n",
    "    is_non_effect_allele_alt = alt_alleles_set.contains(non_effect_allele)\n",
    "\n",
    "    return hl.case() \\\n",
    "        .when(hl.is_missing(mt_subset.GT) & is_effect_allele_ref, 2) \\\n",
    "        .when(hl.is_missing(mt_subset.GT) & is_effect_allele_alt, 0) \\\n",
    "        .when(mt_subset.GT.is_hom_ref() & is_effect_allele_ref, 2) \\\n",
    "        .when(mt_subset.GT.is_hom_var() & is_effect_allele_alt, 2) \\\n",
    "        .when(mt_subset.GT.is_het() & is_effect_allele_ref, 1) \\\n",
    "        .when(mt_subset.GT.is_het() & is_effect_allele_alt, 1) \\\n",
    "        .default(0)\n",
    "\n",
    "def calculate_final_prs(vds, prs_identifier, pgs_weight_path, output_path):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"#####################\")\n",
    "    print(f\"      {prs_identifier}\")\n",
    "    print(\"#####################\")\n",
    "    \n",
    "    \n",
    "    # Construct paths\n",
    "    bucket = os.getenv(\"WORKSPACE_BUCKET\")\n",
    "    \n",
    "    PGS_path             = f'{bucket}/{pgs_weight_path}'\n",
    "    interval_fp          = f\"{bucket}/{output_path}/interval/{prs_identifier}_interval.tsv\"\n",
    "    hail_fp              = f'{bucket}/{output_path}/hail/' \n",
    "    gc_csv_fp            = f'{bucket}/{output_path}/score/{prs_identifier}_scores.csv' \n",
    "    gc_found_csv_fp      = f'{bucket}/{output_path}/score/{prs_identifier}_found_in_aou.csv' \n",
    "    gc_missing_csv_fp    = f'{bucket}/{output_path}/score/{prs_identifier}_missing_from_aou.csv'\n",
    "\n",
    "    # Create and Save intervals for the weight table\n",
    "    \n",
    "    with gcsfs.GCSFileSystem().open(PGS_path, 'rb') as gcs_file:\n",
    "        PGS_df = pd.read_csv(gcs_file)    \n",
    "        \n",
    "    PGS_df['end'] = PGS_df['position']\n",
    "    PGS_interval_df = PGS_df[['contig', 'position', 'end']]\n",
    "    \n",
    "    PGS_interval_df.to_csv(interval_fp, header=False, index=False, sep=\"\\t\")\n",
    "    print(f\"Intervals save as: {interval_fp}\")\n",
    "    \n",
    "    # Import locus intervals and other necessary operations\n",
    "    PGS_sites = hl.import_locus_intervals(interval_fp, reference_genome='GRCh38', skip_invalid_intervals=True)     \n",
    "        \n",
    "    # Step 1: Read PRS weight table from a file in GCS and import as Hail Table\n",
    "    prs_table = hl.import_table(PGS_path,\n",
    "                                types={\"variant_id\":\"tstr\",\n",
    "                                        \"rsid\":\"tstr\",\n",
    "                                        \"weight\":\"tfloat\",\n",
    "                                        \"contig\":\"tstr\",\n",
    "                                        \"position\":\"tint32\",\n",
    "                                        \"effect_allele\":\"tstr\",\n",
    "                                        \"noneffect_allele\":\"tstr\",\n",
    "                                        \"additive\":\"tint32\",\n",
    "                                        \"recessive\":\"tint32\",\n",
    "                                        \"dominant\":\"tint32\"},\n",
    "                                delimiter=',')\n",
    "    prs_table = prs_table.annotate(locus=hl.locus(prs_table.contig, prs_table.position))\n",
    "    prs_table = prs_table.key_by('locus')\n",
    "\n",
    "    # Step 2: filter varaints\n",
    "    vds_prs = hl.vds.filter_intervals(vds, PGS_sites, keep=True)\n",
    "    \n",
    "    # Step 3: Annotate the MatrixTable with the PRS information and calculate effect allele count\n",
    "    mt_prs = vds_prs.variant_data.annotate_rows(prs_info=prs_table[vds_prs.variant_data.locus])\n",
    "    mt_prs = mt_prs.unfilter_entries() \n",
    "    \n",
    "    # Step 4: Calculate effect allele count and multiply by variant weight in a single step\n",
    "    effect_allele_count_expr = calculate_effect_allele_count_na_hom_ref(mt_prs)\n",
    "    mt_prs = mt_prs.annotate_entries(\n",
    "        effect_allele_count=effect_allele_count_expr,\n",
    "        weighted_count=effect_allele_count_expr * mt_prs.prs_info['weight'])\n",
    "    \n",
    "    # Step 5: Sum the weighted counts per sample and count the number of variants with weights per sample\n",
    "    mt_prs = mt_prs.annotate_cols(\n",
    "        sum_weights=hl.agg.sum(mt_prs.weighted_count),\n",
    "        N_variants=hl.agg.count_where(hl.is_defined(mt_prs.weighted_count)))\n",
    "    \n",
    "    # Step 6: Write the PRS scores to a Hail Table\n",
    "    mt_prs.key_cols_by().cols().write(hail_fp, overwrite=True)\n",
    "    \n",
    "    # Step 7: Export the Hail Table to a CSV file\n",
    "    saved_mt = hl.read_table(hail_fp)\n",
    "    print(f\"saved as: {gc_csv_fp}\")\n",
    "    saved_mt.export(gc_csv_fp, header=True, delimiter=',')\n",
    "\n",
    "    # Step 8: Create a Hail Table with found and missing variants\n",
    "    found_variants_table = mt_prs.filter_rows(hl.is_defined(mt_prs.prs_info)).rows()\n",
    "\n",
    "    # Step 9: Extract prs_info for found variants as a DataFrame\n",
    "    found_prs_info_df = found_variants_table.select(found_variants_table.prs_info).to_pandas()\n",
    "\n",
    "    # Step 10: Write the prs_info DataFrame for found variants to CSV\n",
    "    print(f\"Found variants saved as: {gc_found_csv_fp}\")\n",
    "    found_prs_info_df.to_csv(gc_found_csv_fp, header=True, index=False, sep=',')\n",
    "                \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################\n",
      "      PGS004859\n",
      "#####################\n",
      "Intervals save as: gs://fc-secure-e5684327-e720-41ed-979a-b9ae6477b844/prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou/interval/PGS004859_interval.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 01:58:48.996 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n",
      "2024-06-25 01:58:50.041 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'chr' as type str (not specified)\n",
      "  Loading field 'bp' as type str (not specified)\n",
      "  Loading field 'rs_number' as type str (not specified)\n",
      "  Loading field 'effect_allele' as type str (user-supplied)\n",
      "  Loading field 'noneffect_allele' as type str (user-supplied)\n",
      "  Loading field 'weight' as type float64 (user-supplied)\n",
      "  Loading field 'additive' as type int32 (user-supplied)\n",
      "  Loading field 'recessive' as type int32 (user-supplied)\n",
      "  Loading field 'dominant' as type int32 (user-supplied)\n",
      "  Loading field 'contig' as type str (user-supplied)\n",
      "  Loading field 'position' as type int32 (user-supplied)\n",
      "  Loading field 'variant_id' as type str (user-supplied)\n",
      "2024-06-25 01:58:55.888 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-06-25 02:13:26.287 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-06-25 02:14:00.236 Hail: INFO: wrote table with 1107037 rows in 1 partition to hail_temp//__iruid_8976-heY1ELZtFTPZ26S6J5ggNo\n",
      "2024-06-25 02:52:54.861 Hail: INFO: wrote table with 193835 rows in 176 partitions to gs://fc-secure-e5684327-e720-41ed-979a-b9ae6477b844/prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou/hail/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: gs://fc-secure-e5684327-e720-41ed-979a-b9ae6477b844/prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou/score/PGS004859_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:52:57.002 Hail: INFO: merging 177 files totalling 5.0M...\n",
      "2024-06-25 02:52:57.469 Hail: INFO: while writing:\n",
      "    gs://fc-secure-e5684327-e720-41ed-979a-b9ae6477b844/prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou/score/PGS004859_scores.csv\n",
      "  merge time: 465.642ms\n",
      "2024-06-25 02:55:38.676 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-06-25 02:56:10.654 Hail: INFO: wrote table with 1107037 rows in 1 partition to hail_temp//__iruid_15870-Zd0wOLcIqU6auqqJViIfj8\n",
      "[Stage 21:=================================================>(75154 + 1) / 75155]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found variants saved as: gs://fc-secure-e5684327-e720-41ed-979a-b9ae6477b844/prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou/score/PGS004859_found_in_aou.csv\n",
      "CPU times: user 16min, sys: 12.5 s, total: 16min 12s\n",
      "Wall time: 1h 20min 4s\n"
     ]
    }
   ],
   "source": [
    "%time calculate_final_prs(vds_subset, 'PGS004859', 'prs_calculator_tutorial/prs_calculator_hail_vds/vat_check/PGS004859_weights_tabel.csv', 'prs_calculator_tutorial/prs_calculator_hail_vds/calculated_scores/PGS004859_aou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End date: 2024-06-25\n",
      "End time: 03:18:44\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current date and time again\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Record the end time\n",
    "current_date = end_time.date()\n",
    "current_time = end_time.time()\n",
    "\n",
    "# Format the current date\n",
    "formatted_end_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Format the current time\n",
    "formatted_end_time = current_time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# Print the formatted end date and time separately\n",
    "print(\"End date:\", formatted_end_date)\n",
    "print(\"End time:\", formatted_end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
